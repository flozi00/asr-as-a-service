{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cf3dbe",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    Wav2Vec2BertForCTC,\n",
    "    Wav2Vec2BertProcessor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    SeamlessM4TFeatureExtractor,\n",
    "    AutoProcessor,\n",
    ")\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "from accelerate import Accelerator\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from bitsandbytes.optim import PagedAdamW8bit\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c937b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "BASE_MODEL = \"flozi00/distilwhisper-german-canary\"\n",
    "OUT_MODEL = \"distilwhisper-german-canary\"\n",
    "DATASET_NAME=\"flozi00/german-canary-asr-0324\"\n",
    "DATASET_SUBSET=\"default\"\n",
    "AUDIO_PATH=\"audio\"\n",
    "ACCUMULATION_STEPS = 4\n",
    "EPOCHS = 10\n",
    "SAVE_STEPS = 1000\n",
    "LR= 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b7939",
   "metadata": {},
   "source": [
    "Data Class for Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ASRDataCollator:\n",
    "    processor: AutoProcessor\n",
    "    wav_key: str = os.getenv(\"AUDIO_PATH\", \"audio\")\n",
    "    locale_key: str = os.getenv(\"LOCALE_KEY\", \"german\")\n",
    "    text_key: str = os.getenv(\"TEXT_KEY\", \"transkription\")\n",
    "    max_audio_in_seconds: float = float(os.getenv(\"MAX_AUDIO_IN_SECONDS\", 20.0))\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        input_features = []\n",
    "        label_features = []\n",
    "\n",
    "        for i in range(len(features)):\n",
    "            try:\n",
    "                feature = features[i]\n",
    "\n",
    "                myaudio = feature[self.wav_key][\"array\"]\n",
    "                mytext = feature[self.text_key]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            audio_len = int((len(myaudio) / 16000))\n",
    "            if audio_len > self.max_audio_in_seconds:\n",
    "                print(\"skipping audio\")\n",
    "                continue\n",
    "\n",
    "            # Extract the text from the feature and normalize it\n",
    "            mylang = self.locale_key\n",
    "\n",
    "            # Extract the audio features from the audio\n",
    "            extracted = self.processor.feature_extractor(\n",
    "                myaudio,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            # check if feature extractor return input_features or input_values\n",
    "            ft = (\n",
    "                \"input_values\"\n",
    "                if hasattr(extracted, \"input_values\")\n",
    "                else \"input_features\"\n",
    "            )\n",
    "\n",
    "            # append to input_features\n",
    "            input_features.append(\n",
    "                {\n",
    "                    ft: getattr(\n",
    "                        extracted,\n",
    "                        ft,\n",
    "                    )[0]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # set prefix tokens if possible\n",
    "            try:\n",
    "                self.processor.tokenizer.set_prefix_tokens(mylang)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # append to label_features and tokenize\n",
    "            label_features.append(\n",
    "                {\"input_ids\": self.processor.tokenizer(mytext).input_ids}\n",
    "            )\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137e237",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(sentences):\n",
    "    all_text = \" \".join(sentences)\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "\n",
    "def make_ctc_processor(cv_data):\n",
    "    vocab_train = extract_all_chars(cv_data[os.getenv(\"TEXT_KEY\", \"transkription\")])\n",
    "\n",
    "    vocab_list = list(set(vocab_train[\"vocab\"][0]))\n",
    "    vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n",
    "    vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "    del vocab_dict[\" \"]\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    with open(\"vocab.json\", \"w\") as vocab_file:\n",
    "        json.dump(vocab_dict, vocab_file)\n",
    "\n",
    "    tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\n",
    "        \"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\"\n",
    "    )\n",
    "    feature_extractor = SeamlessM4TFeatureExtractor.from_pretrained(BASE_MODEL)\n",
    "    processor = Wav2Vec2BertProcessor(\n",
    "        feature_extractor=feature_extractor, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    return processor\n",
    "\n",
    "def get_model(\n",
    "    model_name: str,\n",
    "    processor_name: str = None,\n",
    "    vocab_size=None,\n",
    "    processor=None,\n",
    "):\n",
    "    kwargs = {}\n",
    "    use_flash_v2 = True\n",
    "    # get the config of the base model and extract the model type from it\n",
    "    conf = AutoConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    ctc_model = False\n",
    "    keys = [\"wav2vec\", \"w2v\"]\n",
    "    for key in keys:\n",
    "        if key in model_name:\n",
    "            ctc_model = True\n",
    "            conf.attention_dropout=0.01\n",
    "            conf.hidden_dropout=0.01\n",
    "            conf.feat_proj_dropout=0.01\n",
    "            conf.mask_time_prob=0.05\n",
    "            conf.layerdrop=0.01\n",
    "            conf.ctc_loss_reduction=\"mean\"\n",
    "            conf.add_adapter = False\n",
    "            conf.num_adapter_layers = 1\n",
    "            use_flash_v2 = False\n",
    "            break\n",
    "    model_class = AutoModelForSpeechSeq2Seq if ctc_model is False else Wav2Vec2BertForCTC\n",
    "    tok_class = AutoProcessor\n",
    "\n",
    "\n",
    "    if processor is None:\n",
    "        processor = tok_class.from_pretrained(\n",
    "            model_name if processor_name is None else processor_name,\n",
    "            legacy=False,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    if use_flash_v2:\n",
    "        kwargs[\"attn_implementation\"] = \"sdpa\"\n",
    "    \n",
    "    if vocab_size is not None:\n",
    "        conf.vocab_size = vocab_size\n",
    "        kwargs[\"ignore_mismatched_sizes\"]=True\n",
    "\n",
    "    model = model_class.from_pretrained(\n",
    "        model_name,\n",
    "        config=conf,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46421276",
   "metadata": {},
   "source": [
    "Trainingsloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def start_training(\n",
    "    model,\n",
    "    processor,\n",
    "    dloader,\n",
    "    OUT_MODEL,\n",
    "    callback=None,\n",
    "):\n",
    "    accelerator = Accelerator(\n",
    "        log_with=\"wandb\",\n",
    "        gradient_accumulation_steps=ACCUMULATION_STEPS,\n",
    "        mixed_precision=\"fp16\",\n",
    "    )\n",
    "    accelerator.init_trackers(\"huggingface\")\n",
    "\n",
    "    # print the number of total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters: {total_params}\")\n",
    "\n",
    "    # print the number of trainable parameters\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameters: {total_trainable_params}\")\n",
    "\n",
    "\n",
    "    optim = PagedAdamW8bit(model.parameters(), lr=LR)\n",
    "\n",
    "    scheduler = ExponentialLR(optim, gamma=0.9995)\n",
    "    model, optim, dloader, scheduler = accelerator.prepare(\n",
    "        model, optim, dloader, scheduler\n",
    "    )\n",
    "\n",
    "    if callback is not None:\n",
    "        eval_ = callback()\n",
    "        if eval_ is not None:\n",
    "            accelerator.log({\"eval_metric\": eval_}, step=0)\n",
    "\n",
    "    def do_save_stuff():\n",
    "        if callback is not None:\n",
    "            eval_ = callback()\n",
    "            if eval_ is not None:\n",
    "                accelerator.log({\"eval_metric\": eval_}, step=index - 1)\n",
    "        model.save_pretrained(\n",
    "            OUT_MODEL,\n",
    "            save_function=accelerator.save,\n",
    "            state_dict=accelerator.get_state_dict(model),\n",
    "            safe_serialization=False,\n",
    "        )\n",
    "        processor.save_pretrained(OUT_MODEL)\n",
    "\n",
    "        try:\n",
    "            model.push_to_hub(\n",
    "                OUT_MODEL,\n",
    "                save_function=accelerator.save,\n",
    "                state_dict=accelerator.get_state_dict(model),\n",
    "                safe_serialization=False,\n",
    "            )\n",
    "            processor.push_to_hub(OUT_MODEL)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not push to hub: {e}\")\n",
    "\n",
    "    index = 1\n",
    "    for epoch in range(EPOCHS):\n",
    "        for data in (pbar := tqdm(dloader)):\n",
    "            if index / ACCUMULATION_STEPS % SAVE_STEPS == 0 and index != 0:\n",
    "                do_save_stuff()\n",
    "\n",
    "            optim.zero_grad()\n",
    "            with accelerator.accumulate(model), accelerator.autocast():\n",
    "                data = {k: v.to(model.device) for k, v in data.items()}\n",
    "                output = model(return_dict=True, **data)\n",
    "                loss = output.loss\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f\"Loss: {loss} LR: {get_lr(optim.optimizer)}\",\n",
    "                    refresh=True,\n",
    "                )\n",
    "                accelerator.log(\n",
    "                    values={\n",
    "                        \"training_loss\": loss,\n",
    "                        \"learning_rate\": get_lr(optim.optimizer),\n",
    "                    },\n",
    "                    step=int(index / ACCUMULATION_STEPS),\n",
    "                )\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_value_(model.parameters(), 0.9)\n",
    "                optim.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            index += 1\n",
    "        do_save_stuff()\n",
    "    do_save_stuff()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8950d",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cv_data = datasets.load_dataset(\n",
    "        DATASET_NAME,\n",
    "        DATASET_SUBSET,\n",
    "        split=\"train\",\n",
    "    ).cast_column(\n",
    "        AUDIO_PATH,\n",
    "        datasets.Audio(sampling_rate=16000, decode=True),\n",
    "    ).with_format(\n",
    "        \"torch\"\n",
    "    )\n",
    "\n",
    "    if \"w2v\" in BASE_MODEL or \"wav2vec\" in BASE_MODEL:\n",
    "        ctc_processor = make_ctc_processor(cv_data)\n",
    "        vocab_size = len(ctc_processor.tokenizer)\n",
    "    else:\n",
    "        ctc_processor = None\n",
    "        vocab_size = None\n",
    "\n",
    "    model, processor = get_model(\n",
    "        model_name=BASE_MODEL, vocab_size=vocab_size, processor=ctc_processor\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model.config.forced_decoder_ids = None\n",
    "        model.config.suppress_tokens = []\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dataloader = ASRDataCollator(processor=processor)\n",
    "\n",
    "    cv_data = cv_data.shuffle(seed=random.randint(0, 1000))\n",
    "    dloader = DataLoader(\n",
    "        cv_data,\n",
    "        collate_fn=dataloader,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    # start the training\n",
    "    start_training(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        dloader=dloader,\n",
    "        OUT_MODEL=OUT_MODEL,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fbe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
